{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-Depth: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a particularly `powerful and flexible` class of **supervised algorithms** for both **classification** and **regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic idea\n",
    "just like 1-layer or multi-layer neural nets\n",
    "- Optimal hyperplane for **linearly separable** patterns\n",
    "- Extend to patterns that are **not linearly separable** by transformations of original data to `map into new space` -- the **Kernel** function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM vs Neural Networks\n",
    "\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/svn-as.mlp.png\">  \n",
    "In simplest manner, **svm without kernel** is a **single neural network neuron** but with different cost function. \n",
    "\n",
    "If you **add a kernel function**, then it is comparable with **2 layer neural nets**.  \n",
    "First layer is able to project data into some other space and next layer classifies the projected data. \n",
    "\n",
    "If you force to have one more layer then you might **ensemble multiple kernel svms** then you mimics **3 layer nn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivating Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Support Vector Machine (SVM) is a `Discriminative classification`,  \n",
    "finds a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall from 1-layer nets : Which Separating Hyperplane?\n",
    "* In general, lots of possible solutions for a,b,c (an **infinite number**!)\n",
    "* SVM finds an **optimal solution**\n",
    "\n",
    "<img align=\"rigth\" style=\"padding-left:10px;\" src=\"figures/hyperplanes.png\" width=\"50%\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An example \n",
    "consider the **simple** case of a **classification task**, in which the two classes of points are well separated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create a simple 2D problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear discriminative classifier\n",
    "* draws a straight **line separating the two sets** of data, and thereby **create a model** for classification.\n",
    "* **Problem**: there is **more than one** possible dividing **line** that can perfectly discriminate between the two classes!\n",
    "\n",
    "We can draw them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem\n",
    "* These are **three *very* different separators** which, nevertheless, perfectly discriminate between these samples.\n",
    "* Depending on which you choose, a **new data point** (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
    "\n",
    "Evidently our simple intuition of \"`drawing a line between classes`\" is **not enough**, and we need to think a bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines: Maximizing the *Margin*\n",
    "\n",
    "* **Intuition**:  \n",
    "rather than simply drawing a **zero-width line** between the classes,  \n",
    "we can draw around each line a **margin** of some width, up to the nearest point.\n",
    "\n",
    "Here is an example of how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum margin\n",
    "In support vector machines, \n",
    "* the **line that maximizes this margin** is the one we will choose as the **optimal model**.\n",
    "* Support vector machines are an example of such a *maximum margin* estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General input/output for SVMs just like for neural nets, but for one important addition…\n",
    "* **Input**: set of (input, output) training pair samples; call the input sample features $x_1, x_2, \\dots x_n$, and the output result $y$.  \n",
    "Typically, there can be lots of input features $x_i$.\n",
    "* **Output**: set of weights $w_i$, one for each feature, whose linear combination predicts the value of $y$. (So far, just like neural nets...)\n",
    "* **Important difference**: SVM uses the optimization of **maximizing the margin** (‘`street width`’) in deciding the separating line(hyperplane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definitions\n",
    "<img align=\"right\" style=\"padding-left:10px;\" src=\"figures/hyperplanes-svm.png\" width=\"40%\">\n",
    "Define the hyperplanes $H$ such that:<br>\n",
    "$w\\cdot x_i +b ≥ +1$ when $y_i = +1$ <br>\n",
    "$w\\cdot x_i +b ≤ -1$ when $y_i = –1$ <br>\n",
    "\n",
    "$H_1$ and $H_2$ are the planes:<br>\n",
    "$H_1$: $w\\cdot x_i +b = +1$<br>\n",
    "$H_2$: $w\\cdot x_i +b = –1$ <br>\n",
    "\n",
    "The points on the planes $H_1$ and $H_2$ are the tips of the Support Vectors.  \n",
    "The plane $H_0$ is the median in between, where $w\\cdot x_i+b =0$\n",
    "* $d+ =$ the shortest distance to the closest positive point\n",
    "* $d- =$ the shortest distance to the closest negative point  \n",
    "The margin of a separating hyperplane is $d+ + d–$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Moving input vectors\n",
    "\n",
    "Moving a support vector moves the decision boundary <br>\n",
    "Moving the other vectors has no effect <br>\n",
    "\n",
    "<img src=\"figures/moving-input-vectors.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximizing the margin (street width)\n",
    "\n",
    "We want a classifier (linear separator) with as big a margin as possible. <img align=\"right\" style=\"padding-left:10px;\" src=\"figures/hyperplanes-svm.png\" width=\"40%\">\n",
    "\n",
    "Recall the distance from a point $(x_0,y_0)$ to a line $Ax+By+c = 0$ is: <br>\n",
    "$$\\frac{|Ax_0 +By_0 +c|}{\\sqrt{(A^2+B^2)}}$$ <br>\n",
    "so,\n",
    "the distance between $H_0$ and $H_1$ is then:<br>\n",
    "$$\\frac{|w\\cdot x+b|}{\\|w\\|}=\\frac{1}{\\|w\\|}$$ <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximizing the margin (street width)\n",
    "\n",
    "...so, the total distance between $H_1$ and $H_2$ is thus: <img align=\"right\" style=\"padding-left:10px;\" src=\"figures/hyperplanes-svm.png\" width=\"40%\">\n",
    "$$\\frac{2}{\\|w\\|}$$\n",
    "\n",
    "In order to **maximize the margin**, we thus need to **minimize** $\\|w\\|$.  <br> \n",
    "With the condition that there are no datapoints between $H_1$ and $H_2$: <br>\n",
    "$\\mathbf{x}_i\\cdot \\mathbf{w}+b ≥ +1$ when $y_i=+1$ <br>\n",
    "$\\mathbf{x}_i\\cdot \\mathbf{w}+b\\leq –1$ when $y_i =–1$ <br> \n",
    "Can be combined into: <br>\n",
    "$$y_i (x_i\\cdot w + b) > 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compute the margin\n",
    "Given a positive example $(x^+, 1)$ and a negative example $(x^−, −1)$ closer to $H$,  \n",
    "we have\n",
    "* $w \\cdot x^+ + b = 1$\n",
    "* $w \\cdot x^- + b = -1$\n",
    "\n",
    "So the margin is computing as follow\n",
    "$$d^+=|dist(x^+,H)|=\\frac{|w\\cdot x^+ +b|}{\\|w\\|}=\\frac{1}{\\|w\\|}$$ <br>\n",
    "$$d^-=|dist(x^-,H)|=\\frac{|w\\cdot x^- +b|}{\\|w\\|}=\\frac{1}{\\|w\\|}$$ <br>\n",
    "* Margin = $2 / \\|w\\|$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support vectors\n",
    "All the $x_i$ in the training set s.t. $|dist(x_i, H)| = 1$ are called **support vectors** for $H$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We now must solve a quadratic programming problem (linear separable)\n",
    "\n",
    "* **Problem is**: minimize $\\|w\\|$, s.t. discrimination boundary is obeyed, \n",
    "* which we can rewrite as (Note this is a quadratic function):\n",
    "$$\\min \\frac{1}{2}\\|\\mathbf{w}\\|^2$$\n",
    "$$y_i (x_i\\cdot \\mathbf{w} + b) \\ge 1  ~~~~~~~ i=1\\dots m$$\n",
    "\n",
    "This is a  quadratic optimization problem with linear constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Find a “good” hyperplane when the problem is not linear separable\n",
    "\n",
    "**Problema**: not all the constraints  $y_i (w \\cdot x_i +b) ≥ 1$ can be satisfied for all $i$ (data are not linear separable)\n",
    "\n",
    "**Soluzione**:  \"`slack`\" variables in a quadratic problem.  \n",
    "Modeling potential errors: introducing **slack** variables $\\xi_i$ (xi)\n",
    "\n",
    "* **no error**: $y_i(w\\cdot x_i + b) ≥ 1 ⇒ \\xi_i = 0$\n",
    "* **error**: \n",
    "  * $w · x_i + b ≥ +1 − ξ_i ~~~~ i : y_i = +1$\n",
    "  * $w · x_i + b ≤ −1 + ξ_i ~~~~ i : y_i = −1$\n",
    "  * $ξ_i ≥ 0 ~~~~ i = 1, ..., m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution\n",
    "\n",
    "$$\\min_{w,b,\\xi_1 ...\\xi_m} \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\#|i : \\xi_i > 1|$$\n",
    "\n",
    "Note: $\\#|i : ξ_i > 1| ≤ m$\n",
    "\n",
    "so we the follow definition of a SVM:\n",
    "\n",
    "$$\\min_{w,b,\\xi_1 ...\\xi_m} \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^m{\\xi_i}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"figures/svn_C_param.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a support vector machine using Scikit-Learn\n",
    "\n",
    "Let's see the result of an actual fit to this data: \n",
    "* we will use Scikit-Learn's support vector classifier to train an SVM model on this data.\n",
    "\n",
    "For the time being, we will use a **linear kernel** and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Utility function\n",
    "To better visualize what's happening here,  \n",
    "let's create a quick convenience **function** that will **plot SVM decision boundaries** for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support vectors\n",
    "This is the **dividing line** that `maximizes the margin between the two sets of points`.  \n",
    "> Notice that a few of the **training points just touch the margin**.  \n",
    "\n",
    "These points are known as the **support vectors**, and give the algorithm its name.\n",
    "\n",
    "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot the model\n",
    "* A key to this **classifier's success** is that for the fit, **only the position of the support vectors matter**; \n",
    "* any points further from the margin which are on the correct side do not modify the fit!  \n",
    "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
    "\n",
    "We can see this, for example, if we **plot the model** learned from the `first 60 points` and `first 120 points` of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One strength of the SVM model\n",
    "* In the left panel, we see the model and the support vectors for **60 training points**.\n",
    "* In the right panel, we have **doubled** the number of **training points**, but the model has not changed: \n",
    "  * the three **support vectors** from the left panel **are still the same** support vectors from the right panel.\n",
    "  \n",
    "This **insensitivity** to the exact behavior of distant points is one of the **strengths of the SVM model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interactive mode\n",
    "If you are running this notebook live, you can use IPython's interactive widgets to view this feature of the SVM model interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "interact(plot_svm, N=[10, 100, 200], ax=fixed(None));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Beyond linear boundaries: Kernel SVM\n",
    "\n",
    "Where SVM becomes extremely powerful is when it is combined with **kernels**.  \n",
    "\n",
    "To motivate the need for kernels, let's look at some data that is **not linearly separable**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Project the data into a higher dimension\n",
    "* It is clear that **no linear discrimination** will *ever* be able to separate this data.\n",
    "* But we can think about how we might **project the data into a higher dimension** such that a linear separator *would* be sufficient.\n",
    "\n",
    "For **example**, one simple projection we could use would be to compute a *radial basis function* centered on the middle clump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.exp(-(X ** 2).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3D Visualization  \n",
    "We can visualize this extra data dimension using a three-dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=[-30,0, 30], azip=(-180, 180),\n",
    "         X=fixed(X), y=fixed(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data becomes linearly separable\n",
    "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *r*=0.7.\n",
    "\n",
    "Here we had to **choose and carefully tune our projection**: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.  \n",
    "**Problem**: we would like to somehow **automatically find the best basis functions** to use.\n",
    "\n",
    "This type of basis function transformation is known as a **kernel transformation**, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
    "\n",
    "In Scikit-Learn, we can apply kernelized SVM simply by changing our **linear kernel** to an **RBF** (radial basis function) kernel, using the ``kernel`` model hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6, gamma='scale')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learn nonlinear decision boundary\n",
    "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
    "\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning the SVM: Softening Margins\n",
    "\n",
    "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
    "\n",
    "But what if your **data has** some amount of **overlap**?\n",
    "\n",
    "For `example`, you may have data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Softening Margins Parameter\n",
    "\n",
    "* SVM allows some of the points to stay **into the margin** if that allows a better fit.\n",
    "* The hardness of the margin is controlled by a **tuning parameter**, most often known as $C$.\n",
    "  * For **very large** $C$, the margin is hard, and points cannot lie in it.\n",
    "  * For **smaller** $C$, the margin is softer, and can grow to encompass some points.\n",
    "\n",
    "The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tune the parameter\n",
    "The optimal value of the $C$ parameter will **depend on your dataset**, and should be **tuned** using \n",
    "* cross-validation \n",
    "* similar procedure (refer back to `Hyperparameters and Model Validation`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Face Recognition\n",
    "\n",
    "As an example of support vector machines in action, let's take a look at the facial recognition problem.\n",
    "\n",
    "We will use the `Labeled Faces in the Wild` (LFW) dataset, which consists of several thousand collated photos of various public figures.\n",
    "\n",
    "A fetcher for the dataset is built into Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's **plot a few** of these **faces** to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each image contains [62×47] or nearly 3,000 pixels.\n",
    "* We could proceed by simply **using each pixel** value as a feature\n",
    "* often it is more effective to use some sort of **preprocessor** to extract more meaningful features; \n",
    "* We will use **Principal Component Analysis** (PCA) to extract 150 fundamental components to feed into our support vector machine classifier.\n",
    "\n",
    "We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Split data\n",
    "For the sake of testing our classifier output, we will split the data into a **training** and **testing** set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid search\n",
    "\n",
    "* Finally, we can use a grid search cross-validation to explore combinations of parameters.\n",
    "* Here we will adjust \n",
    "  * ``C`` (which controls the margin hardness) \n",
    "  * ``gamma`` (which controls the size of the radial basis function kernel), \n",
    "  \n",
    "and determine the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid, cv=5, iid=False)\n",
    "\n",
    "%time grid.fit(Xtrain, ytrain)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimal values\n",
    "\n",
    "The **optimal values** fall toward the **middle of our grid**; \n",
    "* if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.\n",
    "\n",
    "Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Test\n",
    "Let's take a look at a few of the test images along with their predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6)\n",
    "\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    yfit_int=round(yfit[i])\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit_int].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "#     axi.set_ylabel(yfit_int, color='black' if yfit_int == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Results\n",
    "* Out of this small sample, our optimal estimator **mislabeled** a few faces.\n",
    "* We can get a better sense of our estimator's performance using the **classification report**, which lists recovery statistics label by label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, yfit,\n",
    "                            target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "We might also display the confusion matrix between these classes.\n",
    "\n",
    "This helps us get a sense of which labels are likely to be confused by the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machine: advantages\n",
    "\n",
    "We have seen here a brief intuitive introduction to the principals behind support vector machines.\n",
    "These methods are a powerful classification method for a number of reasons:\n",
    "\n",
    "- Their dependence on relatively **few support vectors** means that they are very compact models, and take up very little memory.\n",
    "- Once the model is trained, the **prediction phase is very fast**.\n",
    "- Because they are affected only by points near the margin, they **work well with high-dimensional data**.\n",
    "- Their integration with kernel methods makes them very versatile, able to adapt to many types of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machine: disadvantages\n",
    "\n",
    "However, SVMs have several disadvantages as well:\n",
    "\n",
    "- The scaling with the number of samples $N$ is $\\mathcal{O}[N^3]$ at worst, or $\\mathcal{O}[N^2]$ for efficient implementations. For large numbers of training samples, this **computational cost can be prohibitive**.\n",
    "- The results are **strongly dependent** on a suitable choice for the **softening parameter** $C$. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\n",
    "- The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the ``probability`` parameter of ``SVC``), but this extra estimation is costly.\n",
    "\n",
    "With those traits in mind, I generally only turn to SVMs once \n",
    "* other simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "## Practice \n",
    "* Compare SVC model changing \n",
    "  * the kernel parameter in `'linear', 'rbf', 'poly'`\n",
    "  * the C parameter in [0.01, 0.1, 1, 10, 100]\n",
    "* plot the SVC decision boundary \n",
    "* Use the IRIS dataset \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 0, :2] # two features\n",
    "y = y[y != 0] # two classes problem\n",
    "\n",
    "# write your code here\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
