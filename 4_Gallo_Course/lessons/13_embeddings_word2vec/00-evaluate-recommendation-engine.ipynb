{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Ref\n",
    "https://towardsdatascience.com/recommendation-systems-models-and-evaluation-84944a84fb8e\n",
    "\n",
    "https://towardsdatascience.com/evaluation-metrics-for-recommender-systems-df56c6611093"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to evaluate recommendation engine ?\n",
    "\n",
    "<img src=\"figures/evaluation-metrics.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which Recommendation System (RS) is the best?\n",
    "* **Relevant recommendations** are defined as recommendations of items that the **user has rated positively** in the test data. \n",
    "* **The goal** is to **NOT recommend** all the same products that the **user has bought before**. \n",
    "  * How do you know if your model is doing a good job at suggesting products?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Long Tail\n",
    "\n",
    "* Consider num. of **Interactions** such as clicks, ratings, or purchases \n",
    "* The items in the \"**long tail**\" usually **do not have enough interactions** to accurately be recommended using user-based recommender systems like collaborative filtering \"`head`\". <img align=\"right\" style=\"padding-left:10px;\" src=\"figures/long_tail_final.png\" width=\"70%\">  \n",
    "* Because there are **many** observations of **popular items** in the training data, it is not difficult for a recommender system to learn to **accurately predict these items**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical accuracy metrics\n",
    "\n",
    "Used to evaluate accuracy of a filtering technique \n",
    "* by comparing the **predicted ratings** directly with the **actual user rating**\n",
    "\n",
    "These metrics are good to use when \n",
    "* the recommendations are based on **predicting rating** or **number of transactions**. \n",
    "\n",
    "They give us \n",
    "* a sense of how accurate our **prediction** ratings are, \n",
    "* how accurate our **recommendations** are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Root Mean Square Error (RMSE)\n",
    "\n",
    "$$RMSE= \\sqrt{\\frac{1}{N}\\sum(predicted - actual)^2} $$\n",
    "If a user has given a rating of 5 to a movie and we predicted the rating as 4, then RMSE is 1 . \n",
    "* Lesser the RMSE value, better the recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "$$MAE= \\frac{1}{N}\\sum |predicted - actual| $$\n",
    "* Lesser the MAE value, better the recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision support accuracy metrics\n",
    "\n",
    "* Help users `select items that are more similar among available set of items`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision and Recall\n",
    "* These metrics view prediction procedure as a **binary operation** which distinguishes `good items` from those items that are `not good`. \n",
    "<img src=\"figures/precision-recall.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Precision** - Out of all the recommended items, how many did the user actually like?\n",
    "$$P = \\frac{tp}{tp + fp} = \\frac{\\text{# of our recommendations that are relevant}}{\\text{# of items we recommended}}$$\n",
    "* **Recall** - What proportion of items that a user likes were actually recommended \n",
    "$$R = \\frac{tp}{tp + fn} =  \\frac{ \\text{# of our recommendations that are relevant}}{ \\text{# of all the possible relevant items}}$$\n",
    "* **F-measure** - is the harmonic average of the P and R  \n",
    "F1 best value = 1 (perfect precision and recall) and worst at 0\n",
    "$$F1=2\\frac{ P\\cdot R}{P+R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Precision and Recall **don’t** seem to **care about ordering**. \n",
    "* So instead we use precision and recall at **cutoff k**. \n",
    "* Consider that we make **N recommendations** and consider only the first k element, \n",
    "* then only the first two, then only the first three, etc… these subsets can be indexed by k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"figures/precision-at-4.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Precision, Recall and F1-Measure May Fool You\n",
    "* Ideal recommender (example a – f) vs. Worst-case recommender (ex. g – l )\n",
    "* Four recommendations (R1 – R4) e.g. Precision@4\n",
    "* Ten items with a varying ratio of relevant items (1 – 9 relevant items)\n",
    "* Precision, recall and F1-measure are very sensitive to the ratio of relevant items\n",
    "* They fail to distinguish between an ideal recommender and a worst-case recommender if the ratio of relevant items is varied \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### P@k and R@k\n",
    "\n",
    "Precision and Recall at cutoff $k$, $P@k$, and $R@k$, \n",
    "* considering only the `subset of your recommendations from rank 1 through k`. \n",
    "* The rank of the recommendations is determined by the predicted value. \n",
    "  * For eg., the product with the highest predicted value is ranked 1, the product with the $k$-*th* highest predicted value is ranked k.\n",
    "<img src=\"figures/precision-at-k.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Average Precision\n",
    "If we have to **recommend $N$ items** and there are $m$ **relevant items** in the full space of items, Average Precision AP@N is defined as:\n",
    "\n",
    "$$AP@N=\\frac{1}{m}\\sum_{k=1}^NP@k\\cdot rel(k)$$\n",
    "\n",
    "where $rel(k)$ is just an indicator (0/1) that tells us whether that $k$-th item was relevant and $P@k$ is the precision@k. \n",
    "\n",
    "> AP rewards you for giving correct recommendations,\n",
    "AP rewards you for front-loading the recommendations that are most likely to be correct,\n",
    "AP will never penalize you for adding additional recommendations to your list — just make sure you front-load the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean Average Precision\n",
    "\n",
    "$$MAP@N = \\frac{1}{|U|}\\sum_u(AP@N)_u$$\n",
    "AP applies to single data points, like a single user. MAP@N just goes a step further and averages the AP across all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reciprocal Rank\n",
    "\n",
    "Suppose we have recommended 3 movies to a user, say A, B, C in the given order, but the user only liked movie C. As the rank of movie C is 3, the reciprocal rank will be 1/3 .\n",
    "* Larger the mean reciprocal rank, better the recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MAP at k (Mean Average Precision at cutoff k)\n",
    "\n",
    "Precision at cutoff k is the precision calculated by considering only the subset of your recommendations from rank 1 through k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NDCG (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "The main difference between MAP and NDCG is that MAP assumes that an item is either of interest (or not), while NDCG gives the relevance score .\n",
    "* Let us understand it with an example: suppose out of 10 movies – A to J, we can recommend the first five movies, i.e. A, B, C, D and E while we must not recommend the other 5 movies, i.e., F, G, H, I and J. The recommendation was [A,B,C,D]. \n",
    "* So the NDCG in this case will be 1 as the recommended products are relevant for the userCourses & Articles - where you can explore more about recommendation engine"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
