{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting source \n",
    "Copyright (c) 2015 - 2017 [Sebastian Raschka](sebastianraschka.com)\n",
    "\n",
    "https://github.com/rasbt/python-machine-learning-book\n",
    "\n",
    "[MIT License](https://github.com/rasbt/python-machine-learning-book/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision tree learning\n",
    "\n",
    "Decision tree classifiers are attractive models if we care about **interpretability**.\n",
    "\n",
    "Like the name decision tree suggests, we can think of this model as <br>\n",
    "``breaking down our data by making decisions based on asking a series of questions``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The Basic Algorithm\n",
    "\n",
    "* Start at the root node as parent node\n",
    "* **Split** the parent node at the **feature** $x_i$ to **minimize** the sum of the **child** node **impurities** (maximize information gain)\n",
    "* Assign training samples to new child nodes\n",
    "  * Stop if leave nodes are pure or early stopping criteria is satisfied, \n",
    "  * else repeat steps 1 and 2 for each new child node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stopping Rules\n",
    "\n",
    "* The leaf nodes are **pure**\n",
    "* A maximal node **depth** is reached\n",
    "* Splitting a node does not lead to an **information gain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's consider the following **example** where we use a decision tree to decide upon an activity on a particular day:\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"figures/03_15.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision tree based on real number\n",
    "**Based on the features** in our training set, the ``decision tree model learns a series of questions to infer the class labels`` of the samples. \n",
    "\n",
    "Although the preceding figure illustrated the concept of a decision tree based on **categorical variables**,  \n",
    "the same concept applies if our features are **real numbers** like in the Iris dataset. \n",
    "\n",
    "For example, we could simply define a cut-off value along the sepal width feature axis and ask a binary question \"``sepal width ≥ 2.8 cm?``\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting and Pruning\n",
    "Using the decision algorithm, we start at the **tree root**   \n",
    "and split the data on the feature that results in the **largest information gain** (IG), which will be explained in more detail in the following section. \n",
    "\n",
    "In an **iterative process**,   \n",
    "we can then repeat this splitting procedure at each child node **until the leaves are pure**. \n",
    "\n",
    "This means that the samples at each node all **belong to the same class**. \n",
    "\n",
    "In practice, this can result in a **very deep tree** with many nodes, which can easily lead to <span style=\"color:red\">overfitting</span>. \n",
    "\n",
    "Thus, we typically want to **prune the tree** by setting a limit for the maximal depth of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximizing information gain\n",
    "\n",
    "<img align=\"left\" style=\"padding-right:10px;\"  src=\"figures/decision-tree.svg\" width=\"40%\"> In order to **split** the nodes at the **most informative features**, we need to define an\n",
    "**objective function** that we want to optimize via the tree learning algorithm. \n",
    "\n",
    "Here, our objective function is to **maximize the information gain** at each split, which we\n",
    "define as follows:\n",
    "\n",
    "\n",
    "$$IG(D_p, f) = I( D_p ) − \\sum_{j=1}^m{\\frac{N_j}{N_p}I(D_j)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, $f$ is the **feature** to perform the split, $D_p$ and $D_j$ are the **dataset** of the parent and $j^{th}$ child node, $I$ is our **impurity measure**, $N_p$ is the total number of **samples** at the parent node, and $N_j$ is the number of **samples** in the $j^{th}$ child node. \n",
    "\n",
    "As we can see, the information gain is simply the difference between the impurity of the parent node and the sum of the child node impurities — ``the lower the impurity of the child nodes, the larger the information gain``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary decision tree\n",
    "However, for simplicity and to reduce the combinatorial search space, most libraries (including scikit-learn) implement **binary decision trees**. <br>\n",
    "This means that each parent node is split into two child nodes, $D_{left}$ and $D_{right}$ :\n",
    "\n",
    "$$IG(D_p , a) = I(D_p) − \\frac{N_{left}}{N_p} I(D_{left}) − \\frac{N_{right}}{N_p} I(D_{right})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Impurity measures\n",
    "Now, the <span style=\"color:red\">three</span> **impurity measures** or **splitting criteria** that are commonly used in binary decision trees are \n",
    "- **Gini index** ( $I_G$ ), \n",
    "- **entropy** ( $I_H$ ), \n",
    "- and the **classification error** ( $I_E$ ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Entropy\n",
    "\n",
    "Let's start with the definition of ``entropy`` for all non-empty classes ( $p( i~|~t ) ≠ 0$ ):\n",
    "\n",
    "$$ I_H ( t ) = −\\sum_{i=1}^c{ p ( i ~|~ t )~\\log_2 p ( i ~|~ t )} $$\n",
    "\n",
    "Here, $p ( i~ |~ t )$ is the proportion of the samples that belongs to class $i$ for a particular\n",
    "node t. \n",
    "\n",
    "The **entropy** is therefore **0** if ``all samples at a node belong to the same class``, and the entropy is **maximal** if we have a ``uniform class distribution``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "in a binary class setting, the entropy is 0 if $p ( i = 1~|~ t ) = 1$ or $p ( i = 0~ |~ t ) = 0$ .\n",
    "\n",
    "If the classes are distributed uniformly with $p ( i = 1~|~ t ) = 0.5$ and $p ( i = 0 ~|~ t ) = 0.5$ , the entropy is 1. \n",
    "\n",
    "Therefore, we can say that the entropy criterion attempts to maximize the mutual information in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gini index\n",
    "\n",
    "Intuitively, the ``Gini index`` can be understood as a criterion to minimize the probability of misclassification:\n",
    "\n",
    "$$I_G ( t ) = \\sum_{i=1}^c{ p ( i~|~t ) ( − p ( i~|~t ) )} = 1 − \\sum_{i=1}^c{ p ( i~|~t )^2}$$\n",
    "\n",
    "Similar to entropy, the Gini index is **maximal** if the classes are perfectly mixed, for example, in a binary class setting ( c = 2 ):\n",
    "\n",
    "$$1 − \\sum_{i=1}^c{ 0.5^2} = 0.5$$\n",
    "\n",
    "However, in practice both the Gini index and entropy typically yield very similar results and it is often not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Classification error\n",
    "\n",
    "Another impurity measure is the classification error:\n",
    "\n",
    "$$I_E = 1 − \\max \\{ p ( i~|~t )\\}$$\n",
    "\n",
    "This is a **useful** criterion for **pruning** but ``not recommended for growing a decision tree``, since it is less sensitive to changes in the class probabilities of the nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example\n",
    "We can illustrate this by looking at the two possible splitting scenarios shown in the following figure:\n",
    "\n",
    "<img src=\"./figures/AB-decision-tree.png\" width=\"40%\">\n",
    "\n",
    "We start with a dataset $D_p$ at the parent node that consists of $N_p=80$ divided in 40 samples from class 1 and 40 samples from class 2 that we split into two datasets $D_{left}$ and $D_{right}$, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **information gain** using the **classification error** as a splitting criterion would be the same ( $IG_E = 0.25$ ) in both scenario A and B:\n",
    "\n",
    "$$I_E ( D_p ) = 1 − 0.5 = 0.5$$\n",
    "\n",
    "$$A : I_E ( D_{left} ) = 1 − \\frac{3}{4} = 0.25$$\n",
    "\n",
    "$$A : I_E ( D_{right} ) = 1 − \\frac{3}{4} = 0.25$$\n",
    "\n",
    "$$A : IG_E = 0.5 − \\frac{4}{8} 0.25 − \\frac{4}{8} 0.25 = 0.25$$\n",
    "\n",
    "$$B : I_E ( D_{left} ) = 1 − \\frac{4}{6} = \\frac{1}{3}$$\n",
    "\n",
    "$$B : I_E ( D_{right} ) = 1 − 1 = 0$$\n",
    "\n",
    "$$B : IG_E = 0.5 − \\frac{6}{8}×\\frac{1}{3} − 0 = 0.25$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, the **Gini index** would favor the split in scenario B ( $IG_G = 0.16$ ) over scenario\n",
    "A ( $IG_G = 0.125$ ) , which is indeed more pure:\n",
    "\n",
    "$$I_G ( D_p ) = 1 − \\left( 0.5^2 + 0.5^2 \\right) = 0.5$$\n",
    "\n",
    "$$A : I_G ( D_{left} ) = 1 − \\left(\\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2\\right) = \\frac{3}{8} = 0.375$$\n",
    "\n",
    "$$A : I_G ( D_{right} ) = 1 − \\left(\\left(\\frac{1}{4}\\right)^2 + \\left(\\frac{3}{4}\\right)^2\\right) = \\frac{3}{8} = 0.375$$\n",
    "\n",
    "$$A : I_G = 0.5 − \\frac{4}{8} 0.375 − \\frac{4}{8} 0.375 = 0.125$$\n",
    "\n",
    "$$B : I_G ( D_{left} ) = 1 − \\left(\\left(\\frac{2}{6}\\right)^2 + \\left(\\frac{4}{6}\\right)^2\\right) = \\frac{4}{9} = 0.4$$\n",
    "\n",
    "$$B : I_G ( D_{right} ) = 1 − (1^2 + 0^2 ) = 0$$\n",
    "\n",
    "$$B : I_G = 0.5 − \\frac{6}{8} 0.4 − 0 = 0.16$$\n",
    "\n",
    "\n",
    "Similarly, the **entropy** criterion would favor scenario B ( $IG_H = 0.19$ ) over scenario A ( $IG_H = 0.31$ ) ...\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Plot the impurity indices\n",
    "For a more visual comparison of the three different impurity criteria that we discussed previously, let's **plot** the **impurity indices** for the probability range $[0, 1]$ for class 1. \n",
    "\n",
    "Note that we will also add in a scaled version of the entropy (entropy/2) to observe that the Gini index is an intermediate measure between entropy and the classification error. The code is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gini(p):\n",
    "    return p * (1 - p) + (1 - p) * (1 - (1 - p))\n",
    "\n",
    "\n",
    "def entropy(p):\n",
    "    return - p * np.log2(p) - (1 - p) * np.log2((1 - p))\n",
    "\n",
    "\n",
    "def error(p):\n",
    "    return 1 - np.max([p, 1 - p])\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "ent = [entropy(p) if p != 0 else None for p in x]\n",
    "sc_ent = [e * 0.5 if e else None for e in ent]\n",
    "err = [error(i) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], \n",
    "                          ['Entropy', 'Entropy (scaled)', \n",
    "                           'Gini Impurity', 'Misclassification Error'],\n",
    "                          ['-', '-', '--', '-.'],\n",
    "                          ['black', 'lightgray', 'red', 'green', 'cyan']):\n",
    "    line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),\n",
    "          ncol=3, fancybox=True, shadow=False)\n",
    "\n",
    "ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
    "ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlabel('p(i=1)')\n",
    "plt.ylabel('Impurity Index')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('./figures/impurity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a decision tree\n",
    "\n",
    "Building a decision tree Decision trees can **build complex decision boundaries** by dividing the feature\n",
    "space into rectangles. \n",
    "\n",
    "However, we have to **be careful** since the **deeper** the decision tree, the more **complex** the decision boundary becomes, which can easily result in overfitting. \n",
    "\n",
    "Using ``scikit-learn``, we will now train a decision tree with a **maximum depth** of 3 using **entropy** as a criterion for **impurity**. \n",
    "\n",
    "Although feature scaling may be desired for visualization purposes, note that feature scaling is not a requirement for decision tree algorithms. \n",
    "\n",
    "The code is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# LOAD DATA\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "# SPLIT THE DATASET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Practice \n",
    "* What is the size of data? \n",
    "* What is the size of X_train, X_test, y_train, y_test? \n",
    "* Check that number of samples contained in X and in y are equals\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from plot_decision_regions import plot_decision_regions\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X_combined, y_combined, \n",
    "                      classifier=tree, test_idx=range(105, 150))\n",
    "\n",
    "plt.xlabel('petal length [cm]')\n",
    "plt.ylabel('petal width [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/decision_tree_decision.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "## Practice \n",
    "* Compare current DecisionTreeClassifier with a new version configured with `Gini Impurity`\n",
    "* Compare current DecisionTreeClassifier with a new version configured with `Classification error`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After executing the preceding code example, we get the typical axis-parallel **decision boundaries of the decision tree**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature in ``scikit-learn`` is that it allows us to **export the decision tree** as a .dot file after training, which we can visualize using the ``GraphViz`` program. \n",
    "\n",
    "This program is freely available at http://www.graphviz.org and supported by Linux, Windows, and Mac OS X.\n",
    "\n",
    "* First, we create the .dot file via scikit-learn using the export_graphviz function from the tree submodule, as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree, \n",
    "                out_file='figures/tree.dot', \n",
    "                feature_names=['petal length', 'petal width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have installed GraphViz on our computer, we can **convert** the ``tree.dot`` file into a PNG file by executing the following command from the command line in the location where we saved the tree.dot file:\n",
    "\n",
    "> *dot -Tpng tree.dot -o tree.png*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "Image(filename='./figures/tree.png', width=600) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Trace back the splits\n",
    "\n",
    "Looking at the decision tree figure that we created via GraphViz, we can now nicely trace back the splits that the decision tree determined from our training dataset.\n",
    "\n",
    "We started with **120 samples at the root** and split it into **two child** nodes with 40 and 80 samples each using the **petal** with cut-off **≤ 0.8 cm**. \n",
    "\n",
    "After the first split, we can see that the **left child** node is already **pure** and only contains samples from the **Iris-Setosa class** (entropy = 0). \n",
    "\n",
    "The further splits on the **right** are then used to separate the samples from the **Iris-Versicolor** and **Iris-Virginica** classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note**\n",
    "\n",
    "If you have scikit-learn 0.18 and pydotplus installed (e.g., you can install it via `pip install pydotplus`), you can also show the decision tree directly without creating a separate dot file as shown below. Also note that `sklearn 0.18` offers a few additional options to make the ``decision tree visually more appealing``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "  \n",
    "try:\n",
    "\n",
    "    import pydotplus\n",
    "\n",
    "    dot_data = export_graphviz(\n",
    "    tree, \n",
    "    out_file=None,\n",
    "    # the parameters below are new in sklearn 0.18\n",
    "    feature_names=['petal length', 'petal width'],  \n",
    "    class_names=['setosa', 'versicolor', 'virginica'],  \n",
    "    filled=True,\n",
    "    rounded=True)\n",
    "\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "    display(Image(graph.create_png()))\n",
    "\n",
    "except ImportError:\n",
    "    print('pydotplus is not installed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Practice \n",
    "* Compare current DecisionTreeClassifier with new versions configured with max_depth=2,4,6\n",
    "\n",
    "</dev>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advantages of Decision Tree\n",
    "* **Highly Interpretable** & can be visualized\n",
    "* **Minimal data preprocessing** - missing data handling, normalizing, one-hot-encoding not required\n",
    "* Handle both **neumerical & categorical** values\n",
    "* Supports multi-output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of Decision Tree\n",
    "* **Overfitting** - height of tree kept growing with addition of more data\n",
    "* ``Slight changes in data`` or order of data ``can change the tree``\n",
    "* **Imbalanced classes** datasets creates biased tree so data needs balancing\n",
    "\n",
    "##### The above two limitations are handled by ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pruning\n",
    "* A technique of machine learning which **reduces height** of the tree by chopping off parts of the tree that's not doing anything significant in prediction\n",
    "* Two types of pruning - Prepruning & Postpruning\n",
    "  - **Prepruning** : Don't allow tree to grow beyond this point\n",
    "  - **Postpruning** : Allows tree to grow as much as possible, then prune the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
