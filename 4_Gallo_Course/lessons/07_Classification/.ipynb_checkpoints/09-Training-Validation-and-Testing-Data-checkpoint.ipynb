{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training and Testing Data\n",
    "To evaluate how well our supervised models generalize, we can **split** our data into a training and a test set:\n",
    "\n",
    "<img src=\"figures/train_test_split.svg\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **test data** is a **simulation** of \"**future data**\" which will come into the system during production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle before splitting\n",
    "For iris, the labels in iris are sorted, which means that if we split the data using a proportional split, we will get all of specific labels (0 and 1) and very little of another (2). \n",
    "\n",
    "We want to split as illustrated above, but *after* the data has been **randomly shuffled**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "classifier = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get an accurate simulation of the real world, we will **shuffle** our data **then split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "permutation = rng.permutation(len(X))\n",
    "X, y = X[permutation], y[permutation]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we need to ``split`` the data into **training** and **testing**. <br>\n",
    "Luckily, this is a common pattern in machine learning and scikit-learn has a prebuilt function to split data into training and testing for you. <br>\n",
    "Here we use 50% of the data as training, and 50% testing. <br>\n",
    "80% and 20% is another common split, but there are no hard and fast rules. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.5, random_state=1999)\n",
    "print(\"Labels for training and testing data\")\n",
    "print(train_y)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Never use test set for training!\n",
    "The most important thing is to fairly **evaluate** your system on data it **has not seen during training**!\n",
    "\n",
    "By evaluating our classifier performance on data that has been seen during training, we could get **false confidence** in the power of our system. \n",
    "\n",
    "This might lead to putting a system into production which *fails* at predicting new data!   \n",
    "It is much better to use a train/test split in order to properly see how your trained model is doing on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_X, train_y)\n",
    "pred_y = classifier.predict(test_X)\n",
    "print(\"Fraction Correct\")\n",
    "print(np.sum(pred_y == test_y) / float(len(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualization\n",
    "We can also **visualize** the **correct** and **failed** predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "correct_idx = np.where(pred_y == test_y)[0]\n",
    "print(\"correct\", correct_idx)\n",
    "\n",
    "incorrect_idx = np.where(pred_y != test_y)[0]\n",
    "print(\"incorrect\", incorrect_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot two dimensions\n",
    "colors = [\"darkblue\", \"darkgreen\", \"gray\"]\n",
    "for n, color in enumerate(colors):\n",
    "    idx = np.where(test_y == n)[0]\n",
    "    plt.scatter(test_X[idx, 0], test_X[idx, 1], color=color, label=\"Class %s\" % str(n))\n",
    "    \n",
    "plt.scatter(test_X[incorrect_idx, 0], test_X[incorrect_idx, 1], color=\"darkred\")\n",
    "\n",
    "# Make xlim larger to accommodate legend\n",
    "plt.xlim(3, 9)\n",
    "plt.legend(loc=3)\n",
    "plt.title(\"Iris Classification results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see that the **errors occur in the area where green** (class 1) **and gray** (class 2) **overlap**. \n",
    "\n",
    "This gives us insight about what **features to add** - any feature which helps separate class 1 and class 2 should improve classifier performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Split a dataset in Train and Test using Numpy\n",
    "\n",
    "If we want to split the Iris dataset in two subset, we can use ``numpy.random.permutation`` because we need to **keep track of the indices**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from random import shuffle\n",
    "\n",
    "# We can use NumPy's array indexing:\n",
    "assert len(X) == len(y)\n",
    "p = numpy.random.permutation(len(X))\n",
    "# print(\"indices:\",p)\n",
    "\n",
    "# This will result in creation of separate unison-shuffled arrays.\n",
    "X1 = X[p] \n",
    "y1 = y[p]\n",
    "# print(\"y: \",y)\n",
    "# print(\"y1:\",y1)\n",
    "\n",
    "# Here is a way to split the data into two sets: 80% train, 20% test.\n",
    "split = int(0.8 * len(X))\n",
    "train_X = X1[:split]\n",
    "train_y = y1[:split]\n",
    "\n",
    "test_X = X1[split:]\n",
    "test_y = y1[split:]\n",
    "print(\"Train_y:\", train_y)\n",
    "print(\"Test_y:\", test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using k-fold cross-validation to assess model performance\n",
    "\n",
    "source: [python-machine-learning-book](https://github.com/rasbt/python-machine-learning-book)\n",
    "\n",
    "One of the key steps in building a machine learning model is to ``estimate its performance on data that the model hasn't seen before``.<br>\n",
    "A model can either suffer from **underfitting** if the model is too simple, or it can **overfit** training data  if the model is too complex for the underlying training data. <br>\n",
    "To find an acceptable trade-off, we need to evaluate our model carefully. \n",
    "\n",
    "In this section, you will learn about the useful ``cross-validation techniques`` \n",
    "* **holdout cross-validation** and \n",
    "* **k-fold cross-validation**, \n",
    "\n",
    "which can help us to obtain reliable estimates of the model's generalization error, that is, ``how well the model performs on unseen data``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The holdout method\n",
    "\n",
    "A classic and popular approach for ``estimating the generalization performance`` of machine learning models is **holdout cross-validation**. <br>\n",
    "We split our initial dataset into  \n",
    "* **training** dataset: used for model training \n",
    "* **test** dataset: used to estimate the model performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### model selection: tuning hyperparameters\n",
    "\n",
    "However, in typical machine learning applications, we are also interested in  \n",
    "``tuning and comparing different parameter settings``   \n",
    "to further improve the performance for making predictions on unseen data. \n",
    "\n",
    "This process is called **model selection**, where the term model selection refers to a given classification problem for which we want to select the optimal values of **tuning parameters** (also called **hyperparameters**).\n",
    "\n",
    "However, if we ``reuse the same test dataset`` over and over again during model\n",
    "selection, it will become part of our training data and thus the model will be more\n",
    "likely to **overfit**. <br>\n",
    "Despite this issue, many people still ``use the test set for model selection``, which ``is not a good machine learning practice``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way of using the holdout method for model selection is to separate the data\n",
    "into three parts: <br>\n",
    "* a **training set**, used to fit the different models\n",
    "* a **validation set**, used for the model selection \n",
    "* a **test set**., that the model hasn't seen before during the training and model selection steps\n",
    "\n",
    "\n",
    "<img src=\"./figures/06_02.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How could we split randomly a data set and the corresponding label vector into a ``X_train``, ``X_test``, ``X_val``, ``y_train``, ``y_test``, ``y_val`` with Sklearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We can use sklearn.model_selection.train_test_split twice. \n",
    "# First to split to train, test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# and then split train again into validation and train\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "print(\"Labels for training, validation and testing data\")\n",
    "print(\"TRAIN:\",y_train)\n",
    "print(\"TEST:\",y_test)\n",
    "print(\"VAL:\",y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-fold cross-validation\n",
    "\n",
    "In $k$-fold cross-validation, we ``randomly split`` the training dataset into $k$ folds ``without replacement``, where $k − 1$ folds are used for the model **training** and one fold is used for **testing**. <br>\n",
    "This procedure is repeated $k$ times so that we obtain $k$ models and performance estimates.\n",
    "\n",
    "<img src=\"./figures/06_03.png\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=1, shuffle=False)  \n",
    "classifier = KNeighborsClassifier()\n",
    "scores = []\n",
    "for k, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred_y = classifier.predict(X_test)\n",
    "    score = np.sum(pred_y == y_test) / float(len(y_test))\n",
    "    scores.append(score)\n",
    "    print('Fold: %s, Acc: %.3f' % (k, score))\n",
    "    \n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
